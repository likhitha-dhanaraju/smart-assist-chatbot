{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## GPT-2"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b04ecd54fb4a1e44"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:33:02.556402300Z",
     "start_time": "2024-03-31T17:29:21.221071900Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a63b08499254a72a038fad4eddeeeda"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\likhi\\Documents\\01 Pycharm Code Folder\\edhec\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\likhi\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "text/plain": "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "79eaec3d0cb54bf7a92f1921f4c50d2c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8cdd72b812ca45ad9d238426fd67ee95"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "be0b34f261804f7385e5b37e47763875"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "65fd9c4e88c949179782d9d38ddcc7b0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ccdf29ff5e724c4dafc30dcada7bccd6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f975aa104fc14011a799f67381edea6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "GPT2LMHeadModel(\n  (transformer): GPT2Model(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Conv1D()\n          (c_proj): Conv1D()\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Conv1D()\n          (c_proj): Conv1D()\n          (act): NewGELUActivation()\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 model and tokenizer\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "def generate_response(prompt, max_length=50):\n",
    "    \n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    # Generate response\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(input_ids, max_length=max_length, num_return_sequences=1, pad_token_id=50256)\n",
    "\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return response"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:33:02.569139400Z",
     "start_time": "2024-03-31T17:33:02.556402300Z"
    }
   },
   "id": "29bd0bb07afae5ce"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi there! How can I help you?\n",
      "Chatbot: Hi, how are you?\n",
      "\n",
      "I'm a little bit of a nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a big nerd. I'm a\n",
      "Chatbot: good, but I'm not sure if it's a good idea to have a lot of people who are not going to be able to afford it.\n",
      "\n",
      "\"I think it's a good idea to have a lot of people who are not going\n",
      "Chatbot: what the hell is going on here?\"\n",
      "\n",
      "\"I'm not sure. I'm not sure what's going on here. I'm not sure what's going on here. I'm not sure what's going on here. I'm not sure\n",
      "Chatbot: what are you talking about?\"\n",
      "\n",
      "\"I'm talking about the fact that I'm a woman. I'm a woman. I'm a woman. I'm a woman. I'm a woman. I'm a woman. I'm a woman\n",
      "Chatbot: cancel_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event_event\n",
      "Chatbot: Goodbye!\n"
     ]
    }
   ],
   "source": [
    "print(\"Chatbot: Hi there! How can I help you?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Chatbot:\", response)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T17:33:55.081857700Z",
     "start_time": "2024-03-31T17:33:17.461127900Z"
    }
   },
   "id": "dc5050b8e260184a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Adapted from:\n",
    "https://www.machinecurve.com/index.php/2021/03/16/easy-chatbot-with-dialogpt-machine-learning-and-huggingface-transformers/\n",
    "\"\"\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "class DialoGPT:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str ='microsoft/DialoGPT-large',\n",
    "    ):\n",
    "        if not os.path.exists('./models/dialogpt'):\n",
    "            AutoModelForCausalLM.from_pretrained(model_name).save_pretrained('./models/dialogpt')\n",
    "            AutoTokenizer.from_pretrained(model_name).save_pretrained('./models/dialogpt')\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained('./models/dialogpt')\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained('./models/dialogpt')\n",
    "\n",
    "    def __call__(self, inputs: str) -> str:\n",
    "        inputs_tokenized = self.tokenizer.encode(inputs+ self.tokenizer.eos_token, return_tensors='pt')\n",
    "        reply_ids = self.model.generate(inputs_tokenized, max_length=1250, pad_token_id=self.tokenizer.eos_token_id)\n",
    "        reply = self.tokenizer.decode(reply_ids[:, inputs_tokenized.shape[-1]:][0], skip_special_tokens=True)\n",
    "\n",
    "        return reply\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            user_input = input(\"User: \")\n",
    "            print(\"Bot:\", self(user_input))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T21:54:04.216599500Z",
     "start_time": "2024-03-31T21:54:04.201902500Z"
    }
   },
   "id": "29eab39f1451ca58"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# bot = DialoGPT()\n",
    "# bot.run()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-31T21:54:06.651966100Z"
    }
   },
   "id": "c0fd3e655dad2a1f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dialog-GPT"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3ee857623cb8ba8e"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T22:20:41.706717800Z",
     "start_time": "2024-03-31T22:20:35.763231600Z"
    }
   },
   "id": "3f414b655bac0059"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('microsoft/DialoGPT-large')\n",
    "tokenizer = AutoTokenizer.from_pretrained('microsoft/DialoGPT-large', use_fast=True,\n",
    "                                          padding_side=\"left\", add_eos_token=True, add_bos_token=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T22:40:14.767327800Z",
     "start_time": "2024-03-31T22:40:09.830738500Z"
    }
   },
   "id": "8e51e5c8b80dfe37"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "def generate_response(inputs):\n",
    "\n",
    "    inputs_tokenized = tokenizer.encode(inputs + tokenizer.eos_token, return_tensors='pt')\n",
    "    reply_ids = model.generate(inputs_tokenized, max_length=1250, pad_token_id=tokenizer.eos_token_id)\n",
    "    reply = tokenizer.decode(reply_ids[:, inputs_tokenized.shape[-1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "    return reply"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-31T22:40:14.791932200Z",
     "start_time": "2024-03-31T22:40:14.781448Z"
    }
   },
   "id": "b5fb62f92f60b762"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chatbot: Hi there! How can I help you?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  hello,cna you help me?\n",
      "Chatbot: I can't help you, I'm not a doctor.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  you are rude bro\n",
      "Chatbot: I am not your bro, pal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  alright, but you are still rude SIR\n",
      "Chatbot: I am not.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  ok, can you be a nicer and give longer replies?\n",
      "Chatbot: I can't be nicer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  can I make you be nicer?\n",
      "Chatbot: I'm not sure what you mean.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  is there anything I can do to make you be nice to me?\n",
      "Chatbot: I don't know, I just don't know what to do.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  are you sad chatbot?\n",
      "Chatbot: I am not sad, I am happy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  ok, but you is rude. so fuck you\n",
      "Chatbot: I am not rude.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  do you know any swear words in English?\n",
      "Chatbot: I think it's a reference to the song\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  ok\n",
      "Chatbot: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"Chatbot: Hi there! How can I help you?\")\n",
    "while True:\n",
    "    user_input = input(\"You: \")\n",
    "    print(\"You: \", user_input)\n",
    "    if user_input.lower() == \"exit\":\n",
    "        print(\"Chatbot: Goodbye!\")\n",
    "        break\n",
    "\n",
    "    response = generate_response(user_input)\n",
    "    print(\"Chatbot:\", response)\n",
    "    \n",
    "    time.sleep(15)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-31T22:40:16.164122800Z"
    }
   },
   "id": "50be1f82e8e584a5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "153c70002f99de05"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
