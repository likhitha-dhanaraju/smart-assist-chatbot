{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## getting data from the amazon ask questions section - https://www.amazon.com/ask/questions/asin/B0C6KC23JH/\n",
    "## frame questions based on the attributes of the product"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d974620d9e46dc0"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation for QnA model"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "87b49486592a37a1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing and Importing Libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dcd540d059f85576"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Optionally, you can reset the warning filters later if needed\n",
    "# warnings.resetwarnings()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T18:17:22.188482100Z",
     "start_time": "2024-03-28T18:17:22.132875100Z"
    }
   },
   "id": "eb8d7b000ed6d05d"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data_folder = r\"C:\\\\Users\\\\likhi\\\\Documents\\\\02 Pycharm Datasets\\\\01 Master Thesis\\\\04 Product Data\\\\\"\n",
    "faq_data_folder = r\"C:\\\\Users\\\\likhi\\\\Documents\\\\02 Pycharm Datasets\\\\01 Master Thesis\\\\07 QnA\\\\QnA Limit 100\\\\\"\n",
    "destination_data_dir = r'C:\\\\Users\\\\likhi\\\\Documents\\\\02 Pycharm Datasets\\\\01 Master Thesis\\\\07 QnA\\\\training_test_data\\\\'\n",
    "\n",
    "if not os.path.exists(destination_data_dir):\n",
    "    os.mkdir(destination_data_dir)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T18:17:23.655940Z",
     "start_time": "2024-03-28T18:17:23.615051700Z"
    }
   },
   "id": "ba915a6d5e3dd66d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Getting the Product Data/ Saving Training Files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "617684226cbf4b16"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "## find sentences and remove sentences with see more\n",
    "\n",
    "def clean_descriptions(desc):\n",
    "    #table = str.maketrans(' ', ' ', string.punctuation)\n",
    "    desc = desc.split(' ')\n",
    "    #desc = [word.lower() for word in desc]\n",
    "    #desc = [w.translate(table) for w in desc]\n",
    "    desc = [word for word in desc if len(word)>1]\n",
    "    desc = ' '.join(desc)\n",
    "    # pattern = r'[0-9]'\n",
    "    # desc = re.sub(pattern, '', desc)   \n",
    "    \n",
    "    return desc\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T12:30:54.257544400Z",
     "start_time": "2024-03-27T12:30:54.242283700Z"
    }
   },
   "id": "ca5c1c176adb8bf8"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer\n",
    "\n",
    "model_name =  \"t5-small\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T12:17:54.809826600Z",
     "start_time": "2024-03-27T12:17:51.916824300Z"
    }
   },
   "id": "92428536b1a5e369"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [09:09<00:00,  2.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to process a single product\n",
    "def process_product(product_folder, category_path, product_faq_data):\n",
    "    \n",
    "    product_path = os.path.join(category_path, product_folder)\n",
    "   \n",
    "    try:\n",
    "        data = json.load(open(os.path.join(product_path, product_folder + \".json\"), \"r\", encoding=\"utf-8\"))\n",
    "        product_overview = data.get('product_overview', '')\n",
    "        product_overview_text = \"Product overview is \" + \" \".join([key + \" is \" + value \n",
    "                                                            for key, value in product_overview.items() \n",
    "                                                            ])\n",
    "        \n",
    "\n",
    "        product_description = \"Product description is \" +\",\".join(data.get('description', ''))\n",
    "        # categories = \",\".join(data.get('categories', ''))\n",
    "        product_title = \"Product title is \" + data.get('Title', '')\n",
    "        final_description = product_title + \". \" + product_overview_text + \". \" + product_description\n",
    "    \n",
    "        cleaned_final_description = clean_descriptions(final_description)\n",
    "        \n",
    "        product_overview_questions = [\"What is the \" + key + \" of the product?\" \n",
    "                                      for key in product_overview.keys()]\n",
    "        \n",
    "        product_overview_answers = [value + \" is the \" + key +\" of the product.\" \n",
    "                                    for key, value in product_overview.items()]\n",
    "\n",
    "        if not product_faq_data != list():\n",
    "            faq_questions = [i[0] for i in product_faq_data]\n",
    "            faq_answers = [i[1] for i in product_faq_data]\n",
    "        else:\n",
    "            faq_questions = []\n",
    "            faq_answers = []\n",
    "            \n",
    "        total_questions = product_overview_questions + faq_questions\n",
    "        total_answers = product_overview_answers + faq_answers\n",
    "        \n",
    "        # \n",
    "        # encoding = tokenizer.encode(cleaned_final_description, return_tensors=\"pt\",\n",
    "        #                             max_length=2048, pad_to_max_length=True,\n",
    "        #                             truncation=True)\n",
    "        # \n",
    "        # encoding_shape = encoding.shape[1]\n",
    "        \n",
    "        for question, answer in zip(total_questions, total_answers):\n",
    "            context = cleaned_final_description\n",
    "            context_file.write(context+'\\n')\n",
    "            training_question = question\n",
    "            questions_file.write(training_question + '\\n')\n",
    "            training_answer = answer\n",
    "            answers_file.write(training_answer + '\\n')\n",
    "            \n",
    "            context_and_question_length.append(len(\"context: \" + context + \". Question: \" + training_question))\n",
    "            \n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "        # print(f\"Error processing product {product_folder}: {e}\")\n",
    "        # print(f\"Path to product folder: {product_path}\")\n",
    "        # print(f\"Content of product folder: {os.listdir(product_path)}\")\n",
    "\n",
    "\n",
    "# Initialize lists to keep track of product names\n",
    "all_product_names = []\n",
    "\n",
    "# Tracking the max length of context and question\n",
    "context_and_question_length = []\n",
    "\n",
    "# # Open separate output files for train, validation, and test data\n",
    "context_file = open(os.path.join(destination_data_dir, 'context_file.txt'), 'w', encoding='utf-8')\n",
    "questions_file = open(os.path.join(destination_data_dir, 'questions_file.txt'), 'w', encoding='utf')\n",
    "answers_file = open(os.path.join(destination_data_dir, 'answers_file.txt'), 'w', encoding='utf-8')\n",
    "\n",
    "\n",
    "for category_folder in tqdm(sorted(os.listdir(data_folder))):\n",
    "    product_category_path = os.path.join(data_folder, category_folder)\n",
    "    qna_category_path = os.path.join(faq_data_folder, category_folder + \"_Amazon QnA_data.json\")\n",
    "    \n",
    "    category_faq_data = json.load(open(qna_category_path, 'r', encoding='utf-8'))\n",
    "    \n",
    "    for product_folder in sorted(os.listdir(product_category_path)):\n",
    "        \n",
    "        if product_folder not in all_product_names:                     \n",
    "            all_product_names.append(product_folder)\n",
    "            \n",
    "            try:\n",
    "                product_faq_data = category_faq_data[product_folder]\n",
    "            except:\n",
    "                product_faq_data = list()\n",
    "            \n",
    "            # Process each product using multiprocessing pool\n",
    "            process_product(product_folder, product_category_path, product_faq_data)\n",
    "# \n",
    "# # Close the output files\n",
    "context_file.close()\n",
    "questions_file.close()\n",
    "answers_file.close()\n",
    "\n",
    "print(\"Output files created successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T13:17:08.563387100Z",
     "start_time": "2024-03-27T13:07:58.589892900Z"
    }
   },
   "id": "124d7837df6259e9"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "12610"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(context_and_question_length)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T13:57:15.958407700Z",
     "start_time": "2024-03-27T13:57:15.923282300Z"
    }
   },
   "id": "62ce241f78ad1f20"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Loading the text files"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "75501708a35633df"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "contexts_text = open(os.path.join(destination_data_dir, 'context_file.txt'), 'r', encoding='utf-8').read()\n",
    "answers_text = open(os.path.join(destination_data_dir, 'answers_file.txt'), 'r', encoding='utf-8').read()\n",
    "questions_text = open(os.path.join(destination_data_dir, 'questions_file.txt'), 'r', encoding='utf-8').read()\n",
    "\n",
    "contexts = contexts_text.split('\\n')\n",
    "answers = answers_text.split('\\n')\n",
    "questions = questions_text.split('\\n') "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T18:17:46.587564Z",
     "start_time": "2024-03-28T18:17:30.401396800Z"
    }
   },
   "id": "92b0ca37e87c1b62"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Product title is Perler Beads Assorted Multicolor Fuse Beads for Kids Crafts, 11000 pcs. Product overview is Color is Multicolor Material is Plastic Size is 11,000 Count Brand is Perler Shape is Round Item Weight is 1.69 Pounds Number of Pieces is 11000. Product description is Includes (11000) assorted Perler fuse beads and reusable ironing paper in plastic storage jar. This mega set of 11,000 Perler fuse beads comes with 30 different colors, including toothpaste, pastel lavender, butterscotch, and neon pink. Use your assorted Perler fuse beads with pre-made Perler bead design or get creative and make your own. These multicolor Perler beads are great arts and crafts activity for children. Use Perler pegboards, ironing paper, and an iron to complete your craft. Multicolor Perler beads set suitable for ages and up.'] ['Multicolor is the Color of the product.'] ['What is the Color of the product?']\n"
     ]
    }
   ],
   "source": [
    "print(contexts[:1], answers[:1], questions[:1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T18:17:46.611064Z",
     "start_time": "2024-03-28T18:17:46.594582200Z"
    }
   },
   "id": "4c0759d4cdd49c02"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "len(contexts)\n",
    "\n",
    "count =0 \n",
    "\n",
    "for i in contexts:\n",
    "    if \"Product title is Product\" in i:\n",
    "        count+=1 \n",
    "\n",
    "print(count)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-28T18:19:55.440109100Z",
     "start_time": "2024-03-28T18:19:54.953493900Z"
    }
   },
   "id": "504690688f44ba71"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating PyTorch Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8959ea63f587ee4e"
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, contexts, questions, answers, tokenizer, max_length=15360):\n",
    "        self.contexts = contexts\n",
    "        self.questions = questions\n",
    "        self.answers = answers\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        context = self.contexts[idx]\n",
    "        question = self.questions[idx]\n",
    "        answer = self.answers[idx]\n",
    "        \n",
    "        input_text = f\"question: {question} context: {question}\"\n",
    "        target_text = answer\n",
    "        \n",
    "        # Tokenize input and target text\n",
    "        input_ids = self.tokenizer.encode(input_text, return_tensors=\"pt\", max_length=self.max_length, pad_to_max_length=True, truncation=True)\n",
    "        target_ids = self.tokenizer.encode(target_text, return_tensors=\"pt\", max_length=self.max_length, pad_to_max_length=True, truncation=True)\n",
    "        \n",
    "        # Ensure both input and target sequences have the same length\n",
    "        max_seq_length = max(input_ids.size(1), target_ids.size(1))\n",
    "        \n",
    "        input_ids = input_ids[:, :max_seq_length]\n",
    "        target_ids = target_ids[:, :max_seq_length]\n",
    "                \n",
    "        return {\"input_ids\": input_ids.squeeze(0), \"target_ids\": target_ids.squeeze(0)}\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:00:43.963772400Z",
     "start_time": "2024-03-27T14:00:43.873926300Z"
    }
   },
   "id": "985921c19c7efc2d"
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "data": {
      "text/plain": "<__main__.CustomDataset at 0x236cd3f3210>"
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare custom dataset\n",
    "custom_dataset = CustomDataset(contexts, questions, answers, tokenizer)\n",
    "custom_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:00:52.534455900Z",
     "start_time": "2024-03-27T14:00:52.459424900Z"
    }
   },
   "id": "6680f5032ff753a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating train-test split"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "37e5482b0495a1dd"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "train_size = int(0.8 * len(custom_dataset))  \n",
    "test_size = len(custom_dataset) - train_size \n",
    "\n",
    "train_dataset, test_dataset = random_split(custom_dataset, [train_size, test_size])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:01:01.989246900Z",
     "start_time": "2024-03-27T14:01:01.826825800Z"
    }
   },
   "id": "989f8b8ed855068d"
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "data": {
      "text/plain": "<torch.utils.data.dataset.Subset at 0x236cba27010>"
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:01:08.830093600Z",
     "start_time": "2024-03-27T14:01:08.753389200Z"
    }
   },
   "id": "dbcf0d99b4bfebb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e5d737bfebcba33d"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save custom dataset\n",
    "with open(os.path.join(destination_data_dir, \"QnA_train_dataset.pkl\"), \"wb\") as f:\n",
    "    pickle.dump(train_dataset, f)\n",
    "    \n",
    "# Save custom dataset\n",
    "with open(os.path.join(destination_data_dir, \"QnA_test_dataset.pkl\"), \"wb\") as g:\n",
    "    pickle.dump(test_dataset, g)\n",
    "\n",
    "# # Load custom dataset\n",
    "# with open(\"custom_dataset.pkl\", \"rb\") as f:\n",
    "#     loaded_custom_dataset = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-27T14:03:35.385409300Z",
     "start_time": "2024-03-27T14:02:41.971014600Z"
    }
   },
   "id": "1cf463903b7af6a2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "fc3b9c09ce491956"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
