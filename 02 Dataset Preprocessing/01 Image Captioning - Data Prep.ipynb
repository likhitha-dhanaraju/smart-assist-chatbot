{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation for Image Captioning models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2d70a72a5068f31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing and Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "775b91bb1d8d37bb"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-21T09:41:10.123531200Z",
     "start_time": "2024-03-21T09:41:10.073013100Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import string\n",
    "import random\n",
    "import multiprocessing\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "data_folder = r\"C:\\\\Users\\\\likhi\\\\Documents\\\\02 Pycharm Datasets\\\\01 Master Thesis\\\\04 Product Data\\\\\"\n",
    "destination_image_dir = r'C:\\\\Users\\\\likhi\\\\Documents\\\\02 Pycharm Datasets\\\\01 Master Thesis\\\\06b Image_Captioning_Dataset\\\\imagefolder\\\\'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T09:41:22.026407400Z",
     "start_time": "2024-03-21T09:41:22.013823200Z"
    }
   },
   "id": "fd20530d7f3df1a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "15fe9f8f4f899f9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the train - val - test split of dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9814f9fcc5749c67"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def clean_descriptions(desc):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    desc = desc.split(' ')\n",
    "    desc = [word.lower() for word in desc]\n",
    "    desc = [w.translate(table) for w in desc]\n",
    "    desc = [word for word in desc if len(word)>1]\n",
    "    desc = ' '.join(desc)\n",
    "    \n",
    "    return desc\n",
    "\n",
    "\n",
    "# Function to create train, validation, and test directories\n",
    "def create_directories(root_dir, subdirs):\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(root_dir, subdir), exist_ok=True)\n",
    "\n",
    "# Function to copy files to train, validation, and test directories\n",
    "def copy_to_split(image_path, destination_dir, split):\n",
    "    shutil.copy(image_path, os.path.join(destination_dir, split))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T10:00:24.564294100Z",
     "start_time": "2024-03-18T10:00:24.549148700Z"
    }
   },
   "id": "2e4784f44a294a9b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [1:03:42<00:00, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files and split directories created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to process a single product\n",
    "def process_product(product_folder, category_path, destination_image_dir, train_ratio, val_ratio):\n",
    "    all_images = []\n",
    "    product_path = os.path.join(category_path, product_folder)\n",
    "   \n",
    "    try:\n",
    "        data = json.load(open(os.path.join(product_path, product_folder + \".json\"), \"r\"))\n",
    "        product_overview = data.get('product_overview', '')\n",
    "        product_overview = \",\".join([key + \" is \" + value for key, value in product_overview.items() \n",
    "                            if key.lower() not in ['color', 'colour']])\n",
    "        product_description = \",\".join(data.get('description', ''))\n",
    "        categories = \",\".join(data.get('categories', ''))\n",
    "        product_title = data.get('Title', '')\n",
    "        final_description = product_title + categories + product_description + product_overview\n",
    "        \n",
    "        cleaned_final_description = clean_descriptions(final_description)\n",
    "        \n",
    "        image_names = [i for i in os.listdir(product_path) if '.jpg' in i]\n",
    "        all_images.extend(image_names)\n",
    "\n",
    "        if len(cleaned_final_description) > 0:\n",
    "            for img in image_names:\n",
    "                product_dict = {}  # Create a new dictionary for each iteration\n",
    "                product_dict[\"file_name\"] = img\n",
    "                product_dict[\"text\"] = cleaned_final_description\n",
    "                \n",
    "                # Write the dictionary to the appropriate output file based on the split\n",
    "                rand = random.random()\n",
    "                if rand < train_ratio:\n",
    "                    train_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    copy_to_split(os.path.join(product_path, img), destination_image_dir, 'train')\n",
    "                 \n",
    "                elif rand < train_ratio + val_ratio:\n",
    "                    val_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    copy_to_split(os.path.join(product_path, img), destination_image_dir, 'val')\n",
    "                else:\n",
    "                    test_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    copy_to_split(os.path.join(product_path, img), destination_image_dir, 'test')\n",
    "                \n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        # print(f\"Error processing product {product_folder}: {e}\")\n",
    "        # print(f\"Path to product folder: {product_path}\")\n",
    "        # print(f\"Content of product folder: {os.listdir(product_path)}\")\n",
    "\n",
    "\n",
    "# Initialize lists to keep track of product names\n",
    "all_product_names = []\n",
    "\n",
    "# Define train-val-test split ratios\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Create train, validation, and test directories\n",
    "split_dirs = ['train', 'val', 'test']\n",
    "create_directories(destination_image_dir, split_dirs)\n",
    "\n",
    "# Open separate output files for train, validation, and test data\n",
    "train_output_file = open(os.path.join(destination_image_dir, 'train', 'metadata.jsonl'), 'w')\n",
    "val_output_file = open(os.path.join(destination_image_dir, 'val', 'metadata.jsonl'), 'w')\n",
    "test_output_file = open(os.path.join(destination_image_dir, 'test', 'metadata.jsonl'), 'w')\n",
    "\n",
    "\n",
    "for category_folder in tqdm(sorted(os.listdir(data_folder))):\n",
    "    category_path = os.path.join(data_folder, category_folder)\n",
    "    for product_folder in sorted(os.listdir(category_path)):\n",
    "        \n",
    "        if product_folder not in all_product_names:                     \n",
    "            all_product_names.append(product_folder)\n",
    " \n",
    "            # Process each product using multiprocessing pool\n",
    "            process_product(product_folder, category_path, destination_image_dir, train_ratio, val_ratio)\n",
    "\n",
    "# Close the output files\n",
    "train_output_file.close()\n",
    "val_output_file.close()\n",
    "test_output_file.close()\n",
    "\n",
    "print(\"Output files and split directories created successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T11:04:07.003114300Z",
     "start_time": "2024-03-18T10:00:24.576308Z"
    }
   },
   "id": "a2184c49da72f9cc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "183165\n",
      "train\n",
      "551662\n",
      "val\n",
      "183989\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(destination_image_dir):\n",
    "    if '.jsonl' not in folder:\n",
    "        print(folder)\n",
    "        print(len(os.listdir(os.path.join(destination_image_dir, folder))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T13:10:15.002111500Z",
     "start_time": "2024-03-18T13:08:49.043919800Z"
    }
   },
   "id": "be108af9786c8229"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Fixing json file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "697ccb8a9e079f56"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "folders = ['train', 'val', 'test']\n",
    "\n",
    "json_data_filenames = []\n",
    "json_data = []\n",
    "    \n",
    "for folder in folders:\n",
    "    \n",
    "    print(\"Loading the JSON data from folder - \", folder)\n",
    "    with open(os.path.join(destination_image_dir, folder, 'metadata.jsonl'), 'r') as file:\n",
    "        for line in tqdm(file):\n",
    "            json_data_filenames.append(json.loads(line)['file_name'])\n",
    "            json_data.append(json.loads(line))\n",
    "\n",
    "old_json_data = dict(zip(json_data_filenames, json_data))\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a15a10658e09ec85"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the NEW JSON data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551662/551662 [00:00<00:00, 757890.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving the JSON file!\n",
      "Creating the NEW JSON data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183989/183989 [00:00<00:00, 739133.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving the JSON file!\n",
      "Creating the NEW JSON data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183165/183165 [00:00<00:00, 615666.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished saving the JSON file!\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    \n",
    "    new_json_data = []\n",
    "    \n",
    "    image_files = [i for i in os.listdir(os.path.join(destination_image_dir, folder)) if 'jpg' in i]\n",
    "    print(\"Creating the NEW JSON data for \", folder)\n",
    "    for image in tqdm(image_files):\n",
    "        new_json_data.append(old_json_data[image])\n",
    "    \n",
    "    with open(os.path.join(destination_image_dir, folder, 'metadata.jsonl'), 'w') as f:\n",
    "        f.write(json.dumps(new_json_data) + '\\n')\n",
    "        \n",
    "    print(\"Finished saving the JSON file!\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a7032a34e84888e3"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking if images in the folder are present in the JSON file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d57b8fada732cbd7"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the folder -  train\n",
      "['Product_0545498562_1.jpg', 'Product_0545498562_2.jpg', 'Product_0545498562_5.jpg', 'Product_0545906520_0.jpg', 'Product_0692164308_0.jpg', 'Product_0692164308_1.jpg', 'Product_0692164308_3.jpg', 'Product_0692164308_4.jpg', 'Product_0692164308_5.jpg', 'Product_0767806239_0.jpg']\n",
      "Loaded the JSON file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551662/551662 [55:05<00:00, 166.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing images -  0\n",
      "Checking the folder -  val\n",
      "['Product_043945669X_0.jpg', 'Product_0545449367_0.jpg', 'Product_0545498562_3.jpg', 'Product_0767836316_1.jpg', 'Product_0783226063_0.jpg', 'Product_0783226853_0.jpg', 'Product_0783226926_0.jpg', 'Product_0783227876_0.jpg', 'Product_0790740044_0.jpg', 'Product_0792833198_0.jpg']\n",
      "Loaded the JSON file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183989/183989 [07:02<00:00, 435.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing images -  0\n",
      "Checking the folder -  test\n",
      "['Product_0439740207_0.jpg', 'Product_0545498562_0.jpg', 'Product_0545498562_4.jpg', 'Product_0692164308_2.jpg', 'Product_0767836316_0.jpg', 'Product_0767836316_3.jpg', 'Product_078322592X_1.jpg', 'Product_078322592X_2.jpg', 'Product_0790731460_0.jpg', 'Product_0792838467_0.jpg']\n",
      "Loaded the JSON file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183165/183165 [07:05<00:00, 430.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing images -  0\n"
     ]
    }
   ],
   "source": [
    "folders = ['train', 'val', 'test']\n",
    "\n",
    "for folder in folders:\n",
    "    \n",
    "    print(\"Checking the folder - \", folder)\n",
    "    json_data = json.load(open(os.path.join(destination_image_dir, folder, 'metadata.jsonl'), 'r'))\n",
    "    json_data_filenames = [i['file_name'] for i in json_data]\n",
    "    \n",
    "    print(json_data_filenames[:10])\n",
    "    \n",
    "    print(\"Loaded the JSON file\")\n",
    "    \n",
    "    images = [i for i in os.listdir(os.path.join(destination_image_dir, folder)) if '.jpg' in i]\n",
    "    \n",
    "    count = 0\n",
    "    for image in tqdm(images):\n",
    "        if image not in json_data_filenames:\n",
    "            count+=1\n",
    "            \n",
    "    print(\"Missing images - \", count)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T11:46:01.004842600Z",
     "start_time": "2024-03-21T10:35:16.157272600Z"
    }
   },
   "id": "1a1ae10e2601253b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "bc94744ef1c9c04f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
