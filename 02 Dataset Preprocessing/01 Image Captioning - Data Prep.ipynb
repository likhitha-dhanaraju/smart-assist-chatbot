{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preparation for Image Captioning models"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e2d70a72a5068f31"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Installing and Importing libraries"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "775b91bb1d8d37bb"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:17:29.654595500Z",
     "start_time": "2024-03-25T03:17:29.631652900Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import shutil\n",
    "import string\n",
    "import random\n",
    "from datasets import load_dataset\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "data_folder = r\"C:\\\\Users\\\\likhi\\\\Documents\\\\02 Pycharm Datasets\\\\01 Master Thesis\\\\04 Product Data\\\\\"\n",
    "destination_image_dir = r'C:\\\\Users\\\\likhi\\\\Documents\\\\02 Pycharm Datasets\\\\01 Master Thesis\\\\06b Image_Captioning_Dataset\\\\imagefolder\\\\'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:17:30.229694200Z",
     "start_time": "2024-03-25T03:17:30.206595600Z"
    }
   },
   "id": "fd20530d7f3df1a0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "15fe9f8f4f899f9a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Creating the train - val - test split of dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9814f9fcc5749c67"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def clean_descriptions(desc):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    desc = desc.split(' ')\n",
    "    desc = [word.lower() for word in desc]\n",
    "    desc = [w.translate(table) for w in desc]\n",
    "    desc = [word for word in desc if len(word)>1]\n",
    "    desc = ' '.join(desc)\n",
    "    \n",
    "    return desc\n",
    "\n",
    "\n",
    "# Function to create train, validation, and test directories\n",
    "def create_directories(root_dir, subdirs):\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(root_dir, subdir), exist_ok=True)\n",
    "\n",
    "# Function to copy files to train, validation, and test directories\n",
    "def copy_to_split(image_path, destination_dir, split):\n",
    "    shutil.copy(image_path, os.path.join(destination_dir, split))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:17:26.654220Z",
     "start_time": "2024-03-25T03:17:26.613061900Z"
    }
   },
   "id": "2e4784f44a294a9b"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [1:03:42<00:00, 14.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files and split directories created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to process a single product\n",
    "def process_product(product_folder, category_path, destination_image_dir, train_ratio, val_ratio):\n",
    "    all_images = []\n",
    "    product_path = os.path.join(category_path, product_folder)\n",
    "   \n",
    "    try:\n",
    "        data = json.load(open(os.path.join(product_path, product_folder + \".json\"), \"r\"))\n",
    "        product_overview = data.get('product_overview', '')\n",
    "        product_overview = \",\".join([key + \" is \" + value for key, value in product_overview.items() \n",
    "                            if key.lower() not in ['color', 'colour']])\n",
    "        product_description = \",\".join(data.get('description', ''))\n",
    "        categories = \",\".join(data.get('categories', ''))\n",
    "        product_title = data.get('Title', '')\n",
    "        final_description = product_title + categories + product_description + product_overview\n",
    "        \n",
    "        cleaned_final_description = clean_descriptions(final_description)\n",
    "        \n",
    "        num_words = len(cleaned_final_description.strip().split(\" \"))\n",
    "        \n",
    "        if num_words >= 500:\n",
    "            \n",
    "            cleaned_final_description_words = cleaned_final_description.strip().split(\" \")\n",
    "            cleaned_final_description_words = cleaned_final_description_words[:500]\n",
    "            \n",
    "            cleaned_final_description_words = \" \".join(cleaned_final_description_words)\n",
    "        \n",
    "        image_names = [i for i in os.listdir(product_path) if '.jpg' in i]\n",
    "        all_images.extend(image_names)\n",
    "\n",
    "        if len(cleaned_final_description) > 0:\n",
    "            for img in image_names:\n",
    "                product_dict = {}  # Create a new dictionary for each iteration\n",
    "                product_dict[\"file_name\"] = img\n",
    "                product_dict[\"text\"] = cleaned_final_description\n",
    "                \n",
    "                # Write the dictionary to the appropriate output file based on the split\n",
    "                rand = random.random()\n",
    "                if rand < train_ratio:\n",
    "                    train_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    copy_to_split(os.path.join(product_path, img), destination_image_dir, 'train')\n",
    "                 \n",
    "                elif rand < train_ratio + val_ratio:\n",
    "                    val_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    copy_to_split(os.path.join(product_path, img), destination_image_dir, 'val')\n",
    "                else:\n",
    "                    test_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    copy_to_split(os.path.join(product_path, img), destination_image_dir, 'test')\n",
    "                \n",
    "                \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        # print(f\"Error processing product {product_folder}: {e}\")\n",
    "        # print(f\"Path to product folder: {product_path}\")\n",
    "        # print(f\"Content of product folder: {os.listdir(product_path)}\")\n",
    "\n",
    "\n",
    "# Initialize lists to keep track of product names\n",
    "all_product_names = []\n",
    "\n",
    "# Define train-val-test split ratios\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Create train, validation, and test directories\n",
    "split_dirs = ['train', 'val', 'test']\n",
    "create_directories(destination_image_dir, split_dirs)\n",
    "\n",
    "# Open separate output files for train, validation, and test data\n",
    "train_output_file = open(os.path.join(destination_image_dir, 'train', 'metadata.jsonl'), 'w')\n",
    "val_output_file = open(os.path.join(destination_image_dir, 'val', 'metadata.jsonl'), 'w')\n",
    "test_output_file = open(os.path.join(destination_image_dir, 'test', 'metadata.jsonl'), 'w')\n",
    "\n",
    "\n",
    "for category_folder in tqdm(sorted(os.listdir(data_folder))):\n",
    "    category_path = os.path.join(data_folder, category_folder)\n",
    "    for product_folder in sorted(os.listdir(category_path)):\n",
    "        \n",
    "        if product_folder not in all_product_names:                     \n",
    "            all_product_names.append(product_folder)\n",
    " \n",
    "            # Process each product using multiprocessing pool\n",
    "            process_product(product_folder, category_path, destination_image_dir, train_ratio, val_ratio)\n",
    "\n",
    "# Close the output files\n",
    "train_output_file.close()\n",
    "val_output_file.close()\n",
    "test_output_file.close()\n",
    "\n",
    "print(\"Output files and split directories created successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T11:04:07.003114300Z",
     "start_time": "2024-03-18T10:00:24.576308Z"
    }
   },
   "id": "a2184c49da72f9cc"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "183165\n",
      "train\n",
      "551662\n",
      "val\n",
      "183989\n"
     ]
    }
   ],
   "source": [
    "for folder in os.listdir(destination_image_dir):\n",
    "    if '.jsonl' not in folder:\n",
    "        print(folder)\n",
    "        print(len(os.listdir(os.path.join(destination_image_dir, folder))))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-18T13:10:15.002111500Z",
     "start_time": "2024-03-18T13:08:49.043919800Z"
    }
   },
   "id": "be108af9786c8229"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Checking if images in the folder are present in the JSON file"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d57b8fada732cbd7"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the folder -  train\n",
      "['Product_0545498562_1.jpg', 'Product_0545498562_2.jpg', 'Product_0545498562_5.jpg', 'Product_0545906520_0.jpg', 'Product_0692164308_0.jpg', 'Product_0692164308_1.jpg', 'Product_0692164308_3.jpg', 'Product_0692164308_4.jpg', 'Product_0692164308_5.jpg', 'Product_0767806239_0.jpg']\n",
      "Loaded the JSON file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 551662/551662 [55:05<00:00, 166.87it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing images -  0\n",
      "Checking the folder -  val\n",
      "['Product_043945669X_0.jpg', 'Product_0545449367_0.jpg', 'Product_0545498562_3.jpg', 'Product_0767836316_1.jpg', 'Product_0783226063_0.jpg', 'Product_0783226853_0.jpg', 'Product_0783226926_0.jpg', 'Product_0783227876_0.jpg', 'Product_0790740044_0.jpg', 'Product_0792833198_0.jpg']\n",
      "Loaded the JSON file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183989/183989 [07:02<00:00, 435.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing images -  0\n",
      "Checking the folder -  test\n",
      "['Product_0439740207_0.jpg', 'Product_0545498562_0.jpg', 'Product_0545498562_4.jpg', 'Product_0692164308_2.jpg', 'Product_0767836316_0.jpg', 'Product_0767836316_3.jpg', 'Product_078322592X_1.jpg', 'Product_078322592X_2.jpg', 'Product_0790731460_0.jpg', 'Product_0792838467_0.jpg']\n",
      "Loaded the JSON file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 183165/183165 [07:05<00:00, 430.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing images -  0\n"
     ]
    }
   ],
   "source": [
    "folders = ['train', 'val', 'test']\n",
    "\n",
    "for folder in folders:\n",
    "    \n",
    "    print(\"Checking the folder - \", folder)\n",
    "    json_data = json.load(open(os.path.join(destination_image_dir, folder, 'metadata.jsonl'), 'r'))\n",
    "    json_data_filenames = [i['file_name'] for i in json_data]\n",
    "    \n",
    "    print(json_data_filenames[:10])\n",
    "    \n",
    "    print(\"Loaded the JSON file\")\n",
    "    \n",
    "    images = [i for i in os.listdir(os.path.join(destination_image_dir, folder)) if '.jpg' in i]\n",
    "    \n",
    "    count = 0\n",
    "    for image in tqdm(images):\n",
    "        if image not in json_data_filenames:\n",
    "            count+=1\n",
    "            \n",
    "    print(\"Missing images - \", count)\n",
    "        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-21T11:46:01.004842600Z",
     "start_time": "2024-03-21T10:35:16.157272600Z"
    }
   },
   "id": "1a1ae10e2601253b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a030451b972381e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving the Dataset to PyTorch Dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7805df026a98472"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Path to the folder containing the data\n",
    "root = r\"C:/Users/likhi/Documents/02 Pycharm Datasets/01 Master Thesis/06b Image_Captioning_Dataset/imagefolder/\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:49:38.495735600Z",
     "start_time": "2024-03-25T02:49:38.482610Z"
    }
   },
   "id": "223d278b02064ba8"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200GB\n"
     ]
    }
   ],
   "source": [
    "# Set the environment variable ARROW_LARGE_MEMORY_TEST to '20GB'\n",
    "os.environ['ARROW_LARGE_MEMORY_TEST'] = '200GB'\n",
    "\n",
    "block_size = os.environ.get('ARROW_LARGE_MEMORY_TEST')\n",
    "print(block_size)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T02:49:39.170591700Z",
     "start_time": "2024-03-25T02:49:39.154071700Z"
    }
   },
   "id": "63d83089c6c968e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the train dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6b04da1c50157461"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the paths to your train, validation, and test data directories\n",
    "train_data_path = root +'train/'\n",
    "\n",
    "# Load the train, validation, and test datasets\n",
    "train_dataset = load_dataset(\"imagefolder\", data_dir=train_data_path, split='train')\n",
    "# Optionally, you can inspect the loaded datasets\n",
    "print(\"Train Dataset:\")\n",
    "print(train_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "88d7f51eae8836d3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'train_dataset/'):\n",
    "    os.mkdir(root + 'train_dataset/')\n",
    "    \n",
    "train_dataset.save_to_disk(root + 'train_dataset/')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e4f64a2047cdab3e"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the validation dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce43007edb58102e"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "Resolving data files:   0%|          | 0/183990 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c2833973f2c424e95b56ab96c113c92"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Computing checksums:  26%|##6       | 48336/183990 [00:05<00:14, 9665.37it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ff306fbf24214ebe9ca9f7921714f269"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Generating train split: 0 examples [00:00, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "40c9dcb4d7a640af897495e8025a50ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Dataset:\n",
      "Dataset({\n",
      "    features: ['image', 'text'],\n",
      "    num_rows: 183989\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "val_data_path = root +'val/'\n",
    "\n",
    "val_dataset = load_dataset(\"imagefolder\", data_dir=val_data_path, split='train')\n",
    "\n",
    "print(\"Validation Dataset:\")\n",
    "print(val_dataset)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:49:18.269458500Z",
     "start_time": "2024-03-25T03:42:59.598644500Z"
    }
   },
   "id": "6c8f9c8d90813950"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "data": {
      "text/plain": "Saving the dataset (0/12 shards):   0%|          | 0/183989 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "362ea1792f5947298b6b6344296350f9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if not os.path.exists(root + 'val_dataset_version_2/'):\n",
    "    os.mkdir(root + 'val_dataset_version_2/')\n",
    "    \n",
    "val_dataset.save_to_disk(root + 'val_dataset_version_2/')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-25T03:49:18.274374100Z"
    }
   },
   "id": "ef32ff4481a5b5cb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Saving the test dataset"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2561878ddb7ed8bd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "test_data_path = root +'test/'\n",
    "\n",
    "test_dataset = load_dataset(\"imagefolder\", data_dir=test_data_path, split='train')\n",
    "\n",
    "print(\"Test Dataset:\")\n",
    "print(test_dataset)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "388717f242d2fffc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "if not os.path.exists(root + 'test_dataset/'):\n",
    "    os.mkdir(root + 'test_dataset/')\n",
    "    \n",
    "test_dataset.save_to_disk(root + 'test_dataset/')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "411bb7325c3c9155"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Creating JSON File from scratch for the images"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "abf03c8aada12057"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 265/265 [19:58<00:00,  4.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output files and split directories created successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Function to process a single product\n",
    "def process_product(product_folder, category_path, destination_image_dir, train_ratio, val_ratio):\n",
    "    all_images = []\n",
    "    product_path = os.path.join(category_path, product_folder)\n",
    "   \n",
    "    try:\n",
    "        data = json.load(open(os.path.join(product_path, product_folder + \".json\"), \"r\"))\n",
    "        product_overview = data.get('product_overview', '')\n",
    "        product_overview = \",\".join([key + \" is \" + value for key, value in product_overview.items() \n",
    "                            if key.lower() not in ['color', 'colour']])\n",
    "        product_description = \",\".join(data.get('description', ''))\n",
    "        categories = \",\".join(data.get('categories', ''))\n",
    "        product_title = data.get('Title', '')\n",
    "        final_description = product_title + categories + product_description + product_overview\n",
    "        \n",
    "        cleaned_final_description = clean_descriptions(final_description)\n",
    "        \n",
    "        num_words = len(cleaned_final_description.strip().split(\" \"))\n",
    "        \n",
    "        if num_words >= 500:\n",
    "            \n",
    "            cleaned_final_description_words = cleaned_final_description.strip().split(\" \")\n",
    "            cleaned_final_description_words = cleaned_final_description_words[:500]\n",
    "            \n",
    "            cleaned_final_description_words = \" \".join(cleaned_final_description_words)\n",
    "        \n",
    "        image_names = [i for i in os.listdir(product_path) if '.jpg' in i]\n",
    "        all_images.extend(image_names)\n",
    "\n",
    "        if len(cleaned_final_description) > 0:\n",
    "            for img in image_names:\n",
    "                product_dict = {}  # Create a new dictionary for each iteration\n",
    "                product_dict[\"file_name\"] = img\n",
    "                product_dict[\"text\"] = cleaned_final_description\n",
    "                \n",
    "                \n",
    "                if os.path.exists(os.path.join(destination_image_dir, 'train', img)):\n",
    "                    train_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    \n",
    "                elif  os.path.exists(os.path.join(destination_image_dir, 'val', img)):\n",
    "                    val_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    \n",
    "                else:\n",
    "                    test_output_file.write(json.dumps(product_dict) + '\\n')\n",
    "                    \n",
    "    except Exception as e:\n",
    "        pass\n",
    "        # print(f\"Error processing product {product_folder}: {e}\")\n",
    "        # print(f\"Path to product folder: {product_path}\")\n",
    "        # print(f\"Content of product folder: {os.listdir(product_path)}\")\n",
    "\n",
    "\n",
    "# Initialize lists to keep track of product names\n",
    "all_product_names = []\n",
    "\n",
    "# Define train-val-test split ratios\n",
    "train_ratio = 0.6\n",
    "val_ratio = 0.2\n",
    "\n",
    "# Create train, validation, and test directories\n",
    "split_dirs = ['train', 'val', 'test']\n",
    "create_directories(destination_image_dir, split_dirs)\n",
    "\n",
    "# Open separate output files for train, validation, and test data\n",
    "train_output_file = open(os.path.join(destination_image_dir, 'train', 'metadata.jsonl'), 'w')\n",
    "val_output_file = open(os.path.join(destination_image_dir, 'val', 'metadata.jsonl'), 'w')\n",
    "test_output_file = open(os.path.join(destination_image_dir, 'test', 'metadata.jsonl'), 'w')\n",
    "\n",
    "\n",
    "for category_folder in tqdm(sorted(os.listdir(data_folder))):\n",
    "    category_path = os.path.join(data_folder, category_folder)\n",
    "    for product_folder in sorted(os.listdir(category_path)):\n",
    "        \n",
    "        if product_folder not in all_product_names:                     \n",
    "            all_product_names.append(product_folder)\n",
    " \n",
    "            # Process each product using multiprocessing pool\n",
    "            process_product(product_folder, category_path, destination_image_dir, train_ratio, val_ratio)\n",
    "\n",
    "# Close the output files\n",
    "train_output_file.close()\n",
    "val_output_file.close()\n",
    "test_output_file.close()\n",
    "\n",
    "print(\"Output files created successfully.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-25T03:37:51.142251400Z",
     "start_time": "2024-03-25T03:17:50.988019100Z"
    }
   },
   "id": "aa6d19e603840cdc"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4b8c873760c863e5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
